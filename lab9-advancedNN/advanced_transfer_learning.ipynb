{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albanda/CE888-2023/blob/main/lab9-advancedNN/advanced_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning: feature extraction and fine-tuning a pre-trained model\n",
        "\n",
        "Created by Dr Ana Matran-Fernandez (amatra@essex.ac.uk) for CE888 (Data Science and Decision Making)\n",
        "\n",
        "This notebook contains the starter code and instructions for Unit 9 and illustrates how to freeze layers from a pre-trained neural network so you can use it to either fine-tune it or as a feature extractor for your own problem.\n",
        "\n",
        "#### ***Make sure you change the Runtime to GPU on Colab or training will take ages!***"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6CN0ujpQAQkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6hz0K6poAQkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Tensorflow version\n",
        "import tensorflow as tf\n",
        "#if not tf.__version__.startswith(\"2.15\"):  # Ensure we're running on the correct version of tensorflow\n",
        "#  !pip install tensorflow==2.15\n",
        "#  import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "oEMIa429AVRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "U25P-2hqAQkP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Tensorflow imports\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# VGG 16\n",
        "from tensorflow.keras.applications import VGG16"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uDAzH0XYAQkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset prep\n",
        "\n",
        "We'll be working with the CIFAR10 dataset that we used in the CNN lab."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5E6GgPTFAQkQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "dataset = tf.keras.datasets.cifar10\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "(train_images0, train_labels0), (test_images0, test_labels0) = dataset.load_data()\n",
        "\n",
        "print('Train: X = %s, y = %s' % (train_images0.shape, train_labels0.shape))\n",
        "print('Test: X = %s, y = %s' % (test_images0.shape, test_labels0.shape))\n",
        "\n",
        "train_labels = train_labels0.reshape(-1)\n",
        "test_labels = test_labels0.reshape(-1)\n",
        "\n",
        "num_classification_categories = len(class_names)\n",
        "print('Number of classes = %d' % num_classification_categories)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JQ2vhxk1AQkQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# As we did in the CNN lab, we can plot a few images to get an idea of what the dataset looks like\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    # define subplot\n",
        "    plt.subplot(5,5,i+1)\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(train_images0[i], cmap=plt.get_cmap('gray'))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    if class_names != None:\n",
        "        # Add a label underneath, if we have one...\n",
        "        plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yopGE2ilAQkQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Prep the data\n",
        "\n",
        "# Since we are going to work with VGG16, we note from the documentation that the images need to be preprocessed differently to how we did previously\n",
        "train_images = tf.keras.applications.vgg19.preprocess_input(train_images0)\n",
        "test_images = tf.keras.applications.vgg19.preprocess_input(test_images0)\n",
        "\n",
        "print(train_images.shape, test_images.shape)\n",
        "\n",
        "input_shape = train_images.shape[1:]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BlklbeCmAQkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction\n",
        "\n",
        "Remember that to perform feature extraction we will load the model and keep the convolutional part, adding only the classifier at the end.\n",
        "\n",
        "We're going to load the VGG16 architecture pretrained with the ImageNET dataset. We will freeze all the convolutional layers (this means that when we train our model we WON'T be updating the weights for this layers) and add some dense layers at the top to perform classification on the CIFAR10 dataset.\n",
        "\n",
        "### **YOUR TASK**\n",
        "\n",
        "The model I'm giving you is overfitting a lot.\n",
        "You need to make sure this doesn't happen in your final submission.\n"
      ],
      "metadata": {
        "id": "2WE7ctUU2ORl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYUVnLnCmaeq"
      },
      "source": [
        "# We specify the shape of our inputs\n",
        "input = layers.Input(shape=input_shape)\n",
        "\n",
        "model_ = VGG16(weights='imagenet', include_top=False, input_tensor=input)\n",
        "# include_top=False means that we exclude the last (top) layers (i.e., the head of the network --- the output)\n",
        "# (which are responsible for classifying ImageNET)\n",
        "# So we're only loading the convolutional part of the network (see summary below)\n",
        "model_.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that if we wanted to use some other type of classifier, we can extract the outputs from the model above directly as:\n",
        "\n",
        "```\n",
        "features_training = model.predict(train_images)\n",
        "features_test = model.predict(test_images)\n",
        "# And now we train our own classifier using these!\n",
        "clf = ...\n",
        "clf.fit(features_training, train_labels)\n",
        "y_pred = clf.predict(features_test)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PScBmEOFNlcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now freeze all the layers so that we only need to append our classifier at the top (bottom in the summary)\n",
        "for layer in model_.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Use the generated model\n",
        "last_layer = model_.output  # these are the features!"
      ],
      "metadata": {
        "id": "-m2moCNt3vCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are going to add the fully connected layers onto the model.\n",
        "x = layers.Flatten()(last_layer)\n",
        "\n",
        "# Add fully-connected layers for classification (remember that we need to flatten the features first)\n",
        "# TO DO: THIS IS OUR CLASSIFIER. WITH THE CONFIGURATION I'M GIVING YOU, YOUR MODEL WILL OVERFIT TERRIBLY. YOUR JOB IS TO EDIT THESE LAYERS SO THAT THE VALIDATION ACCURACY IS >0.65 AND THERE'S NO OVERFITTING\n",
        "# HINTS: add a Dropout layer between 'fc1' and 'fc2' and reduce the number of neurons in  'fc1'.\n",
        "# ----- beginning of classifier\n",
        "x = layers.Dense(1024, activation='relu', name='fc1')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(128, activation='relu', name='fc2')(x)\n",
        "# ----- end of classifier\n",
        "\n",
        "prediction = layers.Dense(num_classification_categories, activation='softmax')(x)\n",
        "\n",
        "# What does the model look like now?\n",
        "model = models.Model(inputs=input, outputs=prediction)\n",
        "model.summary()  # TO DO:  you will need this for the Moodle quiz"
      ],
      "metadata": {
        "id": "dL8wg8j54DVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVIP4RvWsQGK"
      },
      "source": [
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 optimizer=keras.optimizers.Adam(),\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "my_callbacks = [ModelCheckpoint(filepath='vgg16_model_featureExtraction.keras', save_best_only=True)]  # DON'T CHANGE THIS LINE\n",
        "\n",
        "# You can change the number of epochs, batch_size...\n",
        "history = model.fit(train_images, train_labels, batch_size=64,\n",
        "                       epochs=10,\n",
        "                       validation_data=(test_images, test_labels),\n",
        "                       callbacks=my_callbacks)\n",
        "\n",
        "np.save('history_featureExtraction.npy', [history.history['accuracy'], history.history['val_accuracy']])  # DON'T CHANGE THIS LINE; you'll need this for the Moodle quiz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obrwH58LrVr6"
      },
      "source": [
        "# Have a look at the training graphs. When your model no longer overfits and your validation accuracy is above 0.65, you can upload it to the Moodle quiz.\n",
        "\n",
        "epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "plt.plot(epochs, history.history['accuracy'], label='train_accuracy')\n",
        "plt.plot(epochs, history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlim([1, None])\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning\n",
        "\n",
        "We will now test a second approach to transfer learning: fine-tuning.\n",
        "Instead of freezing all the convolutional layers, now we'll train the top ones together with the classifier.\n",
        "\n",
        "### **YOUR TASK**\n",
        "\n",
        "Same as before, make sure your model isn't overfitting and you can achieve an accuracy on the validation set above 0.8.\n"
      ],
      "metadata": {
        "id": "ci0gBBwX2VLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We specify the shape of our inputs\n",
        "input = layers.Input(shape=input_shape)\n",
        "\n",
        "# Load VGG16 again\n",
        "model_ = VGG16(weights='imagenet', include_top=False, input_tensor=input)\n",
        "#print(model_.summary())"
      ],
      "metadata": {
        "id": "4mqy-RE62Uq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get last layer so we can add our classifier on top again\n",
        "last_layer = model_.output\n",
        "# Flatten\n",
        "x = layers.Flatten()(last_layer)\n",
        "\n",
        "# Add your classifier\n",
        "# ----- beginning of classifier\n",
        "# TO DO: COPY HERE THE CLASSIFIER CONFIGURATION THAT WORKED FOR YOU IN THE FEATURE EXTRACTION TASK\n",
        "# ----- end of classifier\n",
        "\n",
        "prediction = layers.Dense(num_classification_categories, activation='softmax')(x)\n",
        "\n",
        "# Create model\n",
        "model = models.Model(input, prediction)\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "7ATZ2QpMHIYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We first need to freeze all the layers except for our classifier (as we did for the feature extraction approach)\n",
        "\n",
        "# Choose the layers which are updated while training\n",
        "LAYERS_TO_FREEZE = len(model_.layers)  # DON'T CHANGE THIS\n",
        "for layer in model.layers[:LAYERS_TO_FREEZE]:\n",
        "\tlayer.trainable = False\n",
        "for layer in model.layers[LAYERS_TO_FREEZE:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for i, layer in enumerate(model.layers):\n",
        "  print(layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "PAUCvZCaVf_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (remember that for now we're only training the top layers -- i.e., the ones closest to the output, not the convolutional part!)\n",
        "\n",
        "# For this first part we train for a few epochs only\n",
        "\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "my_callbacks = [ModelCheckpoint(filepath='vgg16_model_fineTuning.keras', save_best_only=True)]  # don't change this line\n",
        "\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,  # this number should be small-ish\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    callbacks=my_callbacks)\n",
        "\n",
        "np.save('history_fineTuning_1.npy', [history.history['accuracy'], history.history['val_accuracy']])  # don't change this line"
      ],
      "metadata": {
        "id": "VFyQBh2QKUu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we've done some training on the top layers (i.e., the classifier), we can do the fine-tuning on the earlier layers\n",
        "\n",
        "# We will now freeze the bottom LAYERS_TO_FREEZE layers and train the remaining top layers\n",
        "\n",
        "LAYERS_TO_FREEZE = len(model_.layers)  # TODO: CHANGE THIS NUMBER (e.g., len(model_.layers) - 5 will train the last 5 layers only)\n",
        "for layer in model.layers[:LAYERS_TO_FREEZE]:\n",
        "\tlayer.trainable = False\n",
        "for layer in model.layers[LAYERS_TO_FREEZE:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for i, layer in enumerate(model.layers):\n",
        "  print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "G0fxuJrSWjyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We recompile the model (or the lines above won't have any effect)\n",
        "# and re-train\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# TODO: Add an early stopping callback here (but don't modify the model checkpoint line!)\n",
        "my_callbacks = [ModelCheckpoint(filepath='vgg16_model_fineTuning.keras', save_best_only=True)]\n",
        "\n",
        "# Train again\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    callbacks=my_callbacks)\n",
        "np.save('history_fineTuning_2.npy', [history.history['accuracy'], history.history['val_accuracy']])  # don't change this line"
      ],
      "metadata": {
        "id": "LSZ3BYsUYG3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have a look at the training graphs.\n",
        "# When your validation accuracy is >0.8 and the difference between training and validation accuracies is <0.1, you can upload it to the Moodle quiz.\n",
        "\n",
        "epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "plt.plot(epochs, history.history['accuracy'], label='train_accuracy')\n",
        "plt.plot(epochs, history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.axhline(y=0.8, color='r', linestyle='-.', label='THRESHOLD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlim([1, None])\n",
        "plt.legend(loc='best')"
      ],
      "metadata": {
        "id": "F6vx1j7fHSsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PvhZIDvkRZXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}