{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albanda/CE888-2023/blob/main/lab8-RNN/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS1IIAS-bcb-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Learning from Sequences: Timeseries and Text\n",
        "\n",
        "Created by Dr Ana Matran-Fernandez (amatra@essex.ac.uk) for CE888 (Data Science and Decision Making)\n",
        "\n",
        "This notebook accompanies Unit 8 and illustrates a classification problem on the IMDB text dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0myiQp20cxRK",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJEG0O8EPIlB",
        "ExecuteTime": {
          "end_time": "2024-03-10T09:02:26.777973500Z",
          "start_time": "2024-03-10T09:02:26.722004Z"
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "#if not tf.__version__.startswith(\"2.15\"):  # Ensure we're running on the correct version of tensorflow\n",
        "#  !pip install tensorflow==2.15"
      ],
      "metadata": {
        "id": "X7RDRNwwXPpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J0bYBQ7_a0L",
        "pycharm": {
          "name": "#%%\n"
        },
        "ExecuteTime": {
          "end_time": "2024-03-10T09:03:25.184356Z",
          "start_time": "2024-03-10T09:02:26.731014400Z"
        }
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0NPDv7X_a0M",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# tensorflow imports\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtv0EjsTbwvm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Data prep\n",
        "\n",
        "We'll try to predict whether the review for a movie is positive or negative looking only at the text of the review.\n",
        "\n",
        "We'll use the IMDB text dataset for this task, which is available on `keras.datasets` and described [here](https://keras.io/api/datasets/imdb/).\n",
        "\n",
        "We'll merge the training and test sets and use 60% for training, 20% for validation, and 20% for testing.\n",
        "\n",
        "\n",
        "As this is a binary classificationp problem, we'll use the binary cross entropy loss function. We will keep track of accuracy when training and evaluating the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-mH-8IGwGo1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def plot_hist_classif(hist):\n",
        "  n_ = len(hist.history['accuracy'])\n",
        "  plt.plot(range(1, n_+1), 100*np.asarray(hist.history['accuracy']), 'bo', label='Accuracy on training set')\n",
        "  plt.plot(range(1, n_+1), 100*np.asarray(hist.history['val_accuracy']), 'b', label='Accuracy on validation set')\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.ylim(0, 100)\n",
        "  plt.axhline(y=50)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TW3Vg49n5V-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "max_features = 15000  # only consider the top 15k words\n",
        "maxlen = 500  # first 500 words of each review\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences', x_train.shape)\n",
        "print(len(x_test), 'test sequences', x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ApTb-jYtSOm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# How balanced is this dataset?\n",
        "print(np.sum(y_train)/len(y_train))\n",
        "print(np.sum(y_test)/len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z53Fbicqr8tm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Merge the two datasets and divide: 60% for training, 20% validation, 20% test\n",
        "x_, y_ = np.concatenate((x_train, x_test)), np.concatenate((y_train, y_test))\n",
        "print(x_.shape, y_.shape)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_, y_, test_size=0.4, random_state=10)  # 60/40 split\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=10)  # 50/50 split\n",
        "print(len(x_train), 'train sequences', x_train.shape)\n",
        "print(len(x_val), 'validation sequences', x_val.shape)\n",
        "print(len(x_test), 'test sequences', x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v_Hy-r8o8pR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "x_train[0][:10]  # numbers!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgEgNSLBpFwh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# We can explore what one of the reviews looks like at this point.\n",
        "\n",
        "# Retrieve the word index file that maps words to indices\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "# Reverse the word index to obtain a dict mapping indices to words (which is what we have)\n",
        "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
        "# Decode the first sequence in the dataset\n",
        "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n",
        "decoded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibmO0V-gcthG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's ensure all sequences have the same length through padding\n",
        "# - shorter reviews are padded with 0's\n",
        "# - longer reviews are cut to our maximum length\n",
        "print('Pad sequences')\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_val shape:', x_val.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzZhXm5gkz_g",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Your turn!\n",
        "\n",
        "Implement a bidirectional RNN that can outperform the LSTM model we saw in the lecture.\n",
        "\n",
        "A starting skeleton could be:\n",
        "\n",
        "*   Embedding layer with an output dimension of 64\n",
        "*   Bidirectional layer with a 32-neuron LSTM layer\n",
        "*   Dense layer with 1 neuron and a sigmoid activation\n",
        "\n",
        "Some suggestions:\n",
        "\n",
        "- Change the sizes of the layers (i.e., the numbers of neurons)\n",
        "- Add dropout\n",
        "- Add other recurrent and/or bidirectional layers\n",
        "\n",
        "Once you have a model that scores over 85% on the validation set, check its performance on the test set and upload it on the code checker in Moodle.\n",
        "\n",
        "Note that the model I'm suggesting above will be able to reach the desired performance, but it will show signs of overfitting. Can you do it by adding regularization (dropout, smaller network) so you don't overfit?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCHOC7dyadaj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "n_epochs = 10  # number of epochs. You can edit this\n",
        "\n",
        "model = models.Sequential()\n",
        "# YOUR CODE HERE!\n",
        "\n",
        "# I RECOMMEND THAT YOU DON'T CHANGE CODE AFTER THIS POINT\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# This model will take a long time to train, so we add an early stopping criterion\n",
        "# I've also added a ModelCheckpoint that will save the best model according to val_accuracy regardless of whether we continue training\n",
        "callbacks = [EarlyStopping(monitor='val_accuracy', patience=1),\n",
        "             ModelCheckpoint(\"model.keras\", save_best_only=True, monitor=\"val_accuracy\", mode='max')]\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=callbacks)\n",
        "plot_hist_classif(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"model.keras\")  # revert to version with best validation performance\n",
        "print('Test Accuracy = %.2f' % model.evaluate(x_test, y_test)[1])"
      ],
      "metadata": {
        "id": "9JVPrxCTYDjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFBmg66_CR7y",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "if model.count_params() > 1000000:\n",
        "    print(\"Due to memory constraints on Moodle, the lab quiz has a 10MB limit on your model size, so you need to use a smaller model to validate through the auto-marker.\")\n",
        "\n",
        "if history.history[\"val_accuracy\"][-1] > 0.85:\n",
        "    print(\"Your model is accurate enough!\")\n",
        "\n",
        "else:\n",
        "    print(\"Accuracy is below the threshold!\")\n",
        "    raise Exception(\"Your model isn't accurate enough to pass the progress checker!\")\n",
        "# Save the model for upload to Moodle Quiz\n",
        "model.save(\"Model.keras\")\n",
        "print('Model saved! You can now upload it to the lab quiz.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WlhjIvZR_a0Q",
        "rloJamyvWPWj",
        "7c-duA1Z_a0R",
        "3ozUAZX__a0R"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}