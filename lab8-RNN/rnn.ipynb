{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS1IIAS-bcb-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Learning from Sequences: Timeseries and Text\n",
    "\n",
    "Created by Dr Ana Matran-Fernandez (amatra@essex.ac.uk) for CE888 (Data Science and Decision Making)\n",
    "\n",
    "This notebook accompanies Unit 8 and illustrates a classification problem on the IMDB text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0myiQp20cxRK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YJEG0O8EPIlB",
    "ExecuteTime": {
     "end_time": "2024-03-10T09:02:26.777973500Z",
     "start_time": "2024-03-10T09:02:26.722004Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1J0bYBQ7_a0L",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-03-10T09:03:25.184356Z",
     "start_time": "2024-03-10T09:02:26.731014400Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U0NPDv7X_a0M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tensorflow imports\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtv0EjsTbwvm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data prep\n",
    "\n",
    "We'll try to predict whether the review for a movie is positive or negative looking only at the text of the review.\n",
    "\n",
    "We'll use the IMDB text dataset for this task, which is available on `keras.datasets` and described [here](https://keras.io/api/datasets/imdb/).\n",
    "\n",
    "We'll merge the training and test sets and use 60% for training, 20% for validation, and 20% for testing.\n",
    "\n",
    "\n",
    "As this is a binary classificationp problem, we'll use the binary cross entropy loss function. We will keep track of accuracy when training and evaluating the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-mH-8IGwGo1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_hist_classif(hist):\n",
    "  n_ = len(hist.history['accuracy'])\n",
    "  plt.plot(range(1, n_+1), 100*np.asarray(hist.history['accuracy']), 'bo', label='Accuracy on training set')\n",
    "  plt.plot(range(1, n_+1), 100*np.asarray(hist.history['val_accuracy']), 'b', label='Accuracy on validation set')\n",
    "  plt.legend()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.ylim(0, 100)\n",
    "  plt.axhline(y=50)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TW3Vg49n5V-",
    "outputId": "4cd71e66-b9fb-4c4e-bc9b-d5f074c9c4c5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n",
      "25000 train sequences (25000,)\n",
      "25000 test sequences (25000,)\n"
     ]
    }
   ],
   "source": [
    "max_features = 15000  # only consider the top 15k words\n",
    "maxlen = 500  # first 500 words of each review\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences', x_train.shape)\n",
    "print(len(x_test), 'test sequences', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ApTb-jYtSOm",
    "outputId": "47e3e2f2-695d-448d-e615-e127fb225594",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# How balanced is this dataset?\n",
    "print(np.sum(y_train)/len(y_train))\n",
    "print(np.sum(y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z53Fbicqr8tm",
    "outputId": "5d8253e1-367e-405f-8c47-797f4a662d54",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000,) (50000,)\n",
      "30000 train sequences (30000,)\n",
      "10000 validation sequences (10000,)\n",
      "10000 test sequences (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Merge the two datasets and divide: 60% for training, 20% validation, 20% test\n",
    "x_, y_ = np.concatenate((x_train, x_test)), np.concatenate((y_train, y_test))\n",
    "print(x_.shape, y_.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_, y_, test_size=0.4, random_state=10)  # 60/40 split\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=10)  # 50/50 split\n",
    "print(len(x_train), 'train sequences', x_train.shape)\n",
    "print(len(x_val), 'validation sequences', x_val.shape)\n",
    "print(len(x_test), 'test sequences', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5v_Hy-r8o8pR",
    "outputId": "e969a678-8019-4bae-dad9-789d616c08a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 11531, 186, 8, 28, 6, 6482, 7, 269, 4042]"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x_train[0][:10]  # numbers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "rgEgNSLBpFwh",
    "outputId": "77052900-f443-4276-ebb9-1972ccf68d3f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"the lilly horror in one is fatale br looks meaningless in bronson be showing as you debut film ample to and ingredients zombi ample they for series and thought she's all manipulate and believing in j show look early last quote desire tight interesting that's kind out is far shelter but of frame br and\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# We can explore what one of the reviews looks like at this point.\n",
    "\n",
    "# Retrieve the word index file that maps words to indices\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words (which is what we have)\n",
    "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibmO0V-gcthG",
    "outputId": "69bd9fbb-f01c-4435-9a91-cb38a0b8d255",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pad sequences\n",
      "x_train shape: (30000, 500)\n",
      "x_val shape: (10000, 500)\n",
      "x_test shape: (10000, 500)\n"
     ]
    }
   ],
   "source": [
    "# Let's ensure all sequences have the same length through padding\n",
    "# - shorter reviews are padded with 0's\n",
    "# - longer reviews are cut to our maximum length\n",
    "print('Pad sequences')\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzZhXm5gkz_g",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Your turn!\n",
    "\n",
    "Implement a bidirectional RNN that can outperform the LSTM model we saw in the lecture.\n",
    "\n",
    "A starting skeleton could be:\n",
    "\n",
    "*   Embedding layer with an output dimension of 64\n",
    "*   Bidirectional layer with a 32-neuron LSTM layer\n",
    "*   Dense layer with 1 neuron and a sigmoid activation\n",
    "\n",
    "Some suggestions:\n",
    "\n",
    "- Change the sizes of the layers (i.e., the numbers of neurons)\n",
    "- Add dropout\n",
    "- Add other recurrent and/or bidirectional layers\n",
    "\n",
    "Once you have a model that scores over 85% on the validation set, check its performance on the test set and upload it on the code checker in Moodle.\n",
    "\n",
    "Note that the model I'm suggesting above will be able to reach the desired performance, but it will show signs of overfitting. Can you do it by adding regularization (dropout, smaller network) so you don't overfit?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCHOC7dyadaj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_ = 10  # number of epochs. You can edit this\n",
    "\n",
    "model = models.Sequential()\n",
    "# YOUR CODE HERE!\n",
    "\n",
    "\n",
    "# I RECOMMEND THAT YOU DON'T CHANGE CODE AFTER THIS POINT\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# This model will take a long time to train, so we add an early stopping criterion\n",
    "# I've also added a ModelCheckpoint that will save the best model according to val_accuracy regardless of whether we continue training\n",
    "callbacks = [EarlyStopping(monitor='val_accuracy', patience=1),\n",
    "             ModelCheckpoint(\"model.keras\", save_best_only=True, monitor=\"val_accuracy\", mode='max')]\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=n_,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=callbacks)\n",
    "plot_hist_classif(history)\n",
    "model = keras.models.load_model(\"model.keras\")\n",
    "print('Test Accuracy = %.2f' % model.evaluate(x_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFBmg66_CR7y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if model.count_params() > 1000000:\n",
    "    print(\"Due to memory constraints on Moodle, the lab quiz has a 10MB limit on your model size, so you need to use a smaller model to validate through the auto-marker.\")\n",
    "\n",
    "if history.history[\"val_accuracy\"][-1] > 0.85:\n",
    "    print(\"Your model is accurate enough!\")\n",
    "\n",
    "else:\n",
    "    print(\"Accuracy is below the threshold!\")\n",
    "    raise Exception(\"Your model isn't accurate enough to pass the progress checker!\")\n",
    "# Save the model into a local folder\n",
    "keras.models.save_model(model, \"Model.h5\",save_format='h5')\n",
    "print('Model saved! You can now upload it to the lab quiz.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv7PyBQQsXav",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WlhjIvZR_a0Q",
    "rloJamyvWPWj",
    "7c-duA1Z_a0R",
    "3ozUAZX__a0R"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
